#Functions to assist in the llm-research folder .py files
#includes but is not limmited to files to open text files
# combine text files

import os
from acl_analyzer import compare_acl
from acl_generator import generate_acl
from core.myabac import parse_abac_file


def write_to_file(filename, lines):

    with open(filename, "w", encoding="utf-8") as f:
        for line in lines:
            f.write(line +"\n")
    
    return



def clear_file(filename):
    with open(filename,"w", encoding="utf-8"):
        pass

    return


def read_entire_file(filename):
    
    with open (filename, "r", encoding="utf-8") as f:
        return f.read().strip()
    
    return

def file_to_text(filename):
    temp_string = ""

    with open (filename, "r", encoding="utf-8") as f:
        temp_string += f.read().strip()
    
    return temp_string

#removed all calls, I just copied the data set and removed the rules.
def read_until_marker(filename, stop_marker):
    lines = []
    with open (filename, "r", encoding="utf-8" ) as f:
        for line in f:
            if line.strip() == stop_marker:
                break
            lines.append(line.rstrip())
        return "\n".join(lines).strip()

def append_to_file(filename, text):
    with open(filename, "a", encoding="utf-8") as f:
        f.write(str(text) + "\n")

def write_text_to_file(filename, text):
    with open(filename, "w", encoding="utf-8") as f:
        f.write(str(text) + "\n")

def append_from_file(dest_file, src_file):
    with open(src_file, "r", encoding="utf-8") as src:
        content = src.read()
    with open(dest_file, "a", encoding="utf-8") as dst:
        dst.write(content)




def prompt_generator(gt_acl_file, llm_abac_policy_file, policy_description_file, complete_request_file ):
   
    prompt_file = "llm-research/engineered-prompt.txt"

    # read inputs
    try:
        prompt = read_entire_file(prompt_file)
        acl = read_entire_file(gt_acl_file)
        attribute_data = read_entire_file(llm_abac_policy_file)
        attribute_description = read_entire_file(policy_description_file)

    except FileNotFoundError as e:
        print(f"Error reading file: {e}")
        return
    except Exception as e:
        print(f"Unexpected read error: {e}")
        return

    # build a single request file
    sections = [
        ("NEW REQUEST", prompt),
        ("ATTRIBUTE_DESCRIPTION", attribute_description),
        ("ATTRIBUTE_DATA", attribute_data),
        ("ACL", acl)
    ]

    combined = []

    for title, content in sections:
        combined.append(f"Section: {title}\n{content}  ##\n")

    write_to_file(complete_request_file, combined)

    
    # combined_text = "\n".join(combined).strip()

    # with open(output_path, "w", encoding="utf-8") as out:
    #     out.write(combined_text + "\n")







def api_loop( gt_acl_file, llm_abac_policy_file,  api_call, ttl):

    session_response = "llm-research/session/session-llm-response.txt"


    print("api loop running")
    #pass prompt request from a .txt file to a string obj (required for the request)
    complete_request = read_entire_file("llm-research/complete-prompt.txt")

    is_match = False
    counter = 0

    while(is_match is False and counter < ttl):
        print(f"\n\niteration # {counter}\n\n")
        payload_text = api_call(complete_request)
    

        #output the abac rules to a file for testing
        if(payload_text is None):
            print(f"skipping iteration: payload not recieved\n")
            continue
        
        with open(session_response, "w", encoding="utf-8") as of:
            of.write(payload_text)


        # generated file 2: this file is a temporary file for each session, it allows us to build an ABAC file with 
            # the rules generated by the LLM
            # we need this file to generate an ACL list
        session_abac_file = "llm-research/session/session-abac.txt"

        # generated file 3: This file is where the llm's ACL list should be stored.
        llm_acl_file = "llm-research/session/session-ACL.txt"

        #clear the file to write in
        clear_file(session_abac_file)

        # write the abac policy (llm version that has no rules) into the session abac file
        append_from_file(session_abac_file, llm_abac_policy_file)

        # write the rules (generated by the LLM) into the session abac file
        append_from_file(session_abac_file, session_response)

        # create abac data structures from the session abac file (the one generated with LLM abac rules)
        user2, res2, rule2 = parse_abac_file(session_abac_file)

        # make a new ACL using the rules given by the LLM
        generate_acl(user2, res2, rule2, llm_acl_file)

        # generated file 4: This file stores the comparison data between the ground truth acl and llm acl
        session_comparison_file  = "llm-research/session/session-comparison.txt"

        #store the comparison in a text object
        temp_text, is_match = compare_acl(gt_acl_file,llm_acl_file)

        #write the comparison to a text file
        write_to_file(session_comparison_file, temp_text)

        #declare and clear file to resend the recursive prompt
        complete_request_file = "llm-research/session/recursive-session-prompt.txt"

        if(counter > 0):
            clear_file(complete_request_file)
            complete_request_file = "llm-research/session/recursive-session-prompt.txt"


            # write the llm respsonse to cache:  this should be a cache of only abac rules. 
                # unless the llm responds with anyhting else.
            string_obj= (f"you have generated this response on iteration # {counter}\n")
            append_to_file("llm-research/session/cache/session-llm-response.cache", string_obj)
            
            append_from_file("llm-research/session/cache/session-llm-response.cache", "llm-research/session/session-llm-response.txt")

            #write the acl list generated by the llm rules to cache
            string_obj =(f"We generated the access control list with your rules. Here is the access control list for iteration {counter}:\n")
            append_to_file("llm-research/session/cache/session-ACL.cache", string_obj)
            append_from_file("llm-research/session/cache/session-ACL.cache", "llm-research/session/session-ACL.txt")

            # write the comparison info to cache
            string_obj=(f"here is a comparison of the ground truth ACL and the ACL that was generated with your rules for iteration {counter} :\n")
            append_to_file("llm-research/session/cache/session-comparison.cache", string_obj)
            append_from_file("llm-research/session/cache/session-comparison.cache", "llm-research/session/session-comparison.txt")

            # let the llm know what iteration we are in
            string_obj= (f"\niteration # {counter} is the most current.\n")
            append_to_file(complete_request_file, string_obj)
            string_obj= (f"\nif you are seeing this request it is because you are doing a poor job. look at all the information im feeding you, take the time to make sure your ACL list will match the one i sent int the orginal prompt as that is the ground truth\n")
            append_to_file(complete_request_file, string_obj)

            # add the LLM ABAC Responses to the new request
            append_from_file(complete_request_file, "llm-research/session/cache/session-llm-response.cache")

            # add the LLM ACL lists into the new request
            append_from_file(complete_request_file, "llm-research/session/cache/session-ACL.cache")

            # add the comparisons to the new request
            append_from_file(complete_request_file, "llm-research/session/cache/session-comparison.cache")

            # add instructions to ask for only abac rules
            string_obj = (f"Reply again with an improvement of the rules you made, reply ONLY with the new set of rules and NOTHING ELSE!\nHere is the original prompt, make sure to stick to it once again.\n")
            append_to_file(complete_request_file, string_obj)

            # add the original prompt
            append_from_file(complete_request_file, "llm-research/complete-prompt.txt")
            
            complete_request = read_entire_file(complete_request_file)

        else:  
            complete_request = read_entire_file("llm-research/complete-prompt.txt")
        print(f"iteration {counter} is complete")
        counter +=1








#AI generated code
def clear_text_files(folder_path):
    for filename in os.listdir(folder_path):
        if filename.endswith((".txt", ".cache")):
            file_path = os.path.join(folder_path, filename)
            with open(file_path, "w", encoding="utf-8"):
                pass
